---
title: "R Notebook"
output: html_notebook
---
Week 11 Assignment
kancharla Naga Jahnavi


```{r}
#loading libraries
library(mlbench)
library(purrr)
library(dplyr)
```


```{r}
library(caret)
```


```{r}
#loading the data and assigning to the ds variable
data("PimaIndiansDiabetes2")
ds <- as.data.frame(na.omit(PimaIndiansDiabetes2))
```


```{r}
# fitting a logistic regression model
logmodel <- glm(diabetes ~ .,
                data = ds,
                family = "binomial")
summary(logmodel)

```
```{r}
cfs <- coefficients(logmodel) ## extract the coefficients
prednames <- variable.names(ds)[-9] ## fetch the names of predictors in a vector
prednames

sz <- 100000000 ## to be used in sampling
##sample(ds$pregnant, size = sz, replace = T)
```

```{r}
dfdata <- map_dfc(prednames,
                  function(nm){ ## function to create a sample-with-replacement for each pred.
                    eval(parse(text = paste0("sample(ds$",nm,
                                             ", size = sz, replace = T)")))
                  }) ## map the sample-generator on to the vector of predictors
## and combine them into a dataframe
```

```{r}
names(dfdata) <- prednames
dfdata

class(cfs[2:length(cfs)])

length(cfs)
```


```{r}
length(prednames)
```

```{r}
sz <- 100000000 ## to be used in sampling
##sample(ds$pregnant, size = sz, replace = T)

dfdata <- map_dfc(prednames,
                  function(nm){ ## function to create a sample-with-replacement for each pred.
                    eval(parse(text = paste0("sample(ds$",nm,
                                             ", size = sz, replace = T)")))
                  }) ## map the sample-generator on to the vector of predictors
## and combine them into a dataframe

names(dfdata) <- prednames
dfdata

class(cfs[2:length(cfs)])

length(cfs)
length(prednames)
## Next, compute the logit values
pvec <- map((1:8),
            function(pnum){
              cfs[pnum+1] * eval(parse(text = paste0("dfdata$",
                                                     prednames[pnum])))
            }) %>% ## create beta[i] * x[i]
  reduce(`+`) + ## sum(beta[i] * x[i])
  cfs[1] ## add the intercept

## exponentiate the logit to obtain probability values of thee outcome variable
dfdata['outcome'] <- ifelse(1/(1 + exp(-(pvec))) > 0.5,
                            1, 0)
```
#xgboost direct
```{r}
#assigning sample sizes
data_sizes<- c(100,1000,10000,100000,1000000,10000000)
```

```{r}
#creating a dataframe to store results
results.direct <- data.frame(
  SampleSize = integer(),
  Accuracy = numeric(),
  TimeTaken = numeric(),
  stringsAsFactors = FALSE
)
```
```{r}

```


```{r}
install.packages("xgboost")
```


```{r}
library(xgboost)
library(caret)
library(dplyr)

# Initializing results
results.direct <- data.frame(SampleSize = integer(),
                      Accuracy = numeric(),
                      TimeTaken = numeric())

# Looping through each sample size
for (sz in data_sizes) {
  cat("\nRunning sample size:", sz, "\n")
  set.seed(123) # for reproducibility

  # Sampling the data
  idx <- sample(1:nrow(dfdata), size = sz, replace = FALSE)
  data_sample <- dfdata[idx, ]

  # Splitting into training and testing (80-20 split)
  trainIndex <- createDataPartition(data_sample$outcome, p = 0.8, list = FALSE)
  trainData <- data_sample[trainIndex, ]
  testData <- data_sample[-trainIndex, ]

  # Preparing  data for xgboost
  dtrain <- xgb.DMatrix(data = as.matrix(select(trainData, -outcome)),
                        label = trainData$outcome)
  dtest <- xgb.DMatrix(data = as.matrix(select(testData, -outcome)),
                       label = testData$outcome)

  # Setting parameters
  params <- list(
    objective = "binary:logistic",
    eval_metric = "error",
    max_depth = 3,
    eta = 0.1
  )

  # Training model and measure time
  start_time <- Sys.time()
  model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 50,
    verbose = 0
  )
  end_time <- Sys.time()

  #
  preds <- predict(model, dtest)
  pred_labels <- ifelse(preds > 0.5, 1, 0)

  # Calculating the  accuracy
  acc <- mean(pred_labels == testData$outcome)

  # Time taken
  time_taken <- as.numeric(difftime(end_time, start_time, units = "secs"))

  # Storing results
  results.direct <- rbind(results, data.frame(SampleSize = sz,
                                       Accuracy = acc,
                                       TimeTaken = time_taken))
}

```

```{r}
results.direct
```

```{r}
# using caret 
# loading the required libraries
library(caret)
library(xgboost)
library(dplyr)

```

```{r}
dataset_sizes<-c(100, 1000, 10000, 100000, 1000000, 10000000)

```

```{r}
#setting up dataframe
caret_results <- data.frame(
  SampleSize = integer(),
  Accuracy = numeric(),
  TimeTaken = numeric(),
  stringsAsFactors = FALSE
)

```

```{r}
#cross validation
train_control <- trainControl(method = "cv", number = 5)

```


```{r}
for (sz in dataset_sizes) {
  
}

```

```{r}
set.seed(123)
idx <- sample(1:nrow(dfdata), size = sz, replace = FALSE)
data_sample <- dfdata[idx, ]

```

```{r}
x <- select(data_sample, -outcome)
y <- as.factor(data_sample$outcome)

```

```{r}
start_time <- Sys.time()

model_caret <- train(
  x = x,
  y = y,
  method = "xgbTree",
  trControl = train_control,
  tuneLength = 3,
  verbosity = 0
)

end_time <- Sys.time()

```
```{r}

```
```{r}

```

```{r}
getwd()
```

```{r}
setwd("C:/Users/jahna/Downloads/week011")
```


```{r}
#to download the csv file
write.csv(dfdata, file = "bootstrapped_data.csv", row.names = FALSE)

```

